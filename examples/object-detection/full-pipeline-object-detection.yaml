apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: object-detection-algorithm-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.12, pipelines.kubeflow.org/pipeline_compilation_time: '2022-05-02T17:19:07.145337',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Testing YOLOv5 on few
      images.", "inputs": [{"default": "ali-bucket-gerard", "name": "bucket_name",
      "optional": true, "type": "String"}, {"default": "us-east-1", "name": "AWS_REGION",
      "optional": true, "type": "String"}], "name": "Object Detection Algorithm"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.12}
spec:
  entrypoint: object-detection-algorithm
  templates:
  - name: object-detection-algorithm
    inputs:
      parameters:
      - {name: AWS_REGION}
      - {name: bucket_name}
    dag:
      tasks:
      - {name: preprocessing, template: preprocessing}
      - name: test-object-detection
        template: test-object-detection
        dependencies: [preprocessing]
        arguments:
          parameters:
          - {name: AWS_REGION, value: '{{inputs.parameters.AWS_REGION}}'}
          - {name: bucket_name, value: '{{inputs.parameters.bucket_name}}'}
          - {name: preprocessing-feedback, value: '{{tasks.preprocessing.outputs.parameters.preprocessing-feedback}}'}
  - name: preprocessing
    container:
      args: ['----output-paths', /tmp/outputs/feedback/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def Preprocessing():\n\n    # You should upload your dataset to s3\n\n  \
        \  # import os\n    # import boto3\n\n    # conn_s3 = boto3.client('s3', region_name=AWS_REGION)\n\
        \n    # # Images names list\n    # filenames = os.listdir(data_path)\n\n \
        \   # # Upload all images to s3\n    # for filename in filenames:\n    # \
        \    conn_s3.upload_file(os.path.join(data_path, filename), \n    #      \
        \                   bucket_name, \n    #                         os.path.join(output_path,\
        \ filename))\n\n    from collections import namedtuple\n    feedback_msg =\
        \ 'Done! Data are on S3.'\n    func_output = namedtuple('MyOutput', ['feedback'])\n\
        \    return func_output(feedback_msg)\n\ndef _serialize_str(str_value: str)\
        \ -> str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of str.'.format(\n            str(str_value),\
        \ str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser\
        \ = argparse.ArgumentParser(prog='Preprocessing', description='')\n_parser.add_argument(\"\
        ----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args\
        \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
        , [])\n\n_outputs = Preprocessing(**_parsed_args)\n\n_output_serializers =\
        \ [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n\
        \    try:\n        os.makedirs(os.path.dirname(output_file))\n    except OSError:\n\
        \        pass\n    with open(output_file, 'w') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: 494280055936.dkr.ecr.us-east-1.amazonaws.com/object-detection:latest
    outputs:
      parameters:
      - name: preprocessing-feedback
        valueFrom: {path: /tmp/outputs/feedback/data}
      artifacts:
      - {name: preprocessing-feedback, path: /tmp/outputs/feedback/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["----output-paths", {"outputPath": "feedback"}], "command": ["sh",
          "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def Preprocessing():\n\n    # You should
          upload your dataset to s3\n\n    # import os\n    # import boto3\n\n    #
          conn_s3 = boto3.client(''s3'', region_name=AWS_REGION)\n\n    # # Images
          names list\n    # filenames = os.listdir(data_path)\n\n    # # Upload all
          images to s3\n    # for filename in filenames:\n    #     conn_s3.upload_file(os.path.join(data_path,
          filename), \n    #                         bucket_name, \n    #                         os.path.join(output_path,
          filename))\n\n    from collections import namedtuple\n    feedback_msg =
          ''Done! Data are on S3.''\n    func_output = namedtuple(''MyOutput'', [''feedback''])\n    return
          func_output(feedback_msg)\n\ndef _serialize_str(str_value: str) -> str:\n    if
          not isinstance(str_value, str):\n        raise TypeError(''Value \"{}\"
          has type \"{}\" instead of str.''.format(\n            str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Preprocessing'',
          description='''')\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\",
          type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = Preprocessing(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "494280055936.dkr.ecr.us-east-1.amazonaws.com/object-detection:latest"}},
          "name": "Preprocessing", "outputs": [{"name": "feedback", "type": "String"}]}',
        pipelines.kubeflow.org/component_ref: '{}'}
  - name: test-object-detection
    container:
      args: [--msg, '{{inputs.parameters.preprocessing-feedback}}', --bucket-name,
        '{{inputs.parameters.bucket_name}}', --AWS-REGION, '{{inputs.parameters.AWS_REGION}}']
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def test_object_detection(msg, bucket_name, AWS_REGION):

            import os
            import boto3
            import subprocess

            subprocess.call("python3 test.py --data coco128.yaml --weights yolov5s.pt --img 640 --batch-size 2", shell=True)

            print(msg)
            #print(os.listdir('runs/test/exp'))

            # Upload results to s3
            conn_s3 = boto3.client('s3', region_name=AWS_REGION)
            output_path = 'runs/test/exp/'

            for filename in os.listdir(output_path):
                path = os.path.join(output_path, filename)
                conn_s3.upload_file(path, bucket_name, os.path.join("object-detection/results/", filename))

        import argparse
        _parser = argparse.ArgumentParser(prog='Test object detection', description='')
        _parser.add_argument("--msg", dest="msg", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--bucket-name", dest="bucket_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--AWS-REGION", dest="AWS_REGION", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = test_object_detection(**_parsed_args)
      image: 494280055936.dkr.ecr.us-east-1.amazonaws.com/object-detection:latest
    inputs:
      parameters:
      - {name: AWS_REGION}
      - {name: bucket_name}
      - {name: preprocessing-feedback}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.12
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--msg", {"inputValue": "msg"}, "--bucket-name", {"inputValue":
          "bucket_name"}, "--AWS-REGION", {"inputValue": "AWS_REGION"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def test_object_detection(msg, bucket_name,
          AWS_REGION):\n\n    import os\n    import boto3\n    import subprocess\n\n    subprocess.call(\"python3
          test.py --data coco128.yaml --weights yolov5s.pt --img 640 --batch-size
          2\", shell=True)\n\n    print(msg)\n    #print(os.listdir(''runs/test/exp''))\n\n    #
          Upload results to s3\n    conn_s3 = boto3.client(''s3'', region_name=AWS_REGION)\n    output_path
          = ''runs/test/exp/''\n\n    for filename in os.listdir(output_path):\n        path
          = os.path.join(output_path, filename)\n        conn_s3.upload_file(path,
          bucket_name, os.path.join(\"object-detection/results/\", filename))\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Test object detection'',
          description='''')\n_parser.add_argument(\"--msg\", dest=\"msg\", type=str,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--bucket-name\",
          dest=\"bucket_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--AWS-REGION\",
          dest=\"AWS_REGION\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = test_object_detection(**_parsed_args)\n"],
          "image": "494280055936.dkr.ecr.us-east-1.amazonaws.com/object-detection:latest"}},
          "inputs": [{"name": "msg"}, {"name": "bucket_name"}, {"name": "AWS_REGION"}],
          "name": "Test object detection"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"AWS_REGION": "{{inputs.parameters.AWS_REGION}}",
          "bucket_name": "{{inputs.parameters.bucket_name}}", "msg": "{{inputs.parameters.preprocessing-feedback}}"}'}
  arguments:
    parameters:
    - {name: bucket_name, value: ali-bucket-gerard}
    - {name: AWS_REGION, value: us-east-1}
  serviceAccountName: pipeline-runner
